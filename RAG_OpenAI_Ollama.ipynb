{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG OpenAI Ollama - Medical Q&A System\n",
        "\n",
        "##  My Hardware Setup\n",
        "\n",
        "This project runs on my development machine:\n",
        "\n",
        "- **CPU**: Intel i7-13620H (13th Gen, 10 cores)\n",
        "- **GPU**: RTX 4070 (8GB VRAM)\n",
        "- **RAM**: 32GB DDR4\n",
        "- **Storage**: SSD\n",
        "\n",
        "##  Why I Chose Llama3.2:1B\n",
        "\n",
        "I selected **Llama3.2:1B** model based on my hardware:\n",
        "\n",
        "### Memory Optimization\n",
        "- **My RTX 4070 has 8GB VRAM** - perfect for 1B parameter model\n",
        "- Llama3.2:1B needs ~2-3GB VRAM, leaves room for embeddings\n",
        "- Larger models (7B+) would exceed VRAM and use slow CPU\n",
        "\n",
        "### Performance\n",
        "- **Fast inference** (~1-2 seconds per response)\n",
        "- **Low latency** for interactive use\n",
        "- **Works well with RAG** - context is pre-filtered\n",
        "\n",
        "### Local Benefits\n",
        "- **No API costs** - runs entirely local\n",
        "- **Privacy** - all data stays on my machine\n",
        "- **Always available** - no internet dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import faiss\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "import numpy as np\n",
        "import openai"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
