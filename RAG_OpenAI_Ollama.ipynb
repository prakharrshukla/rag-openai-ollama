{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG OpenAI Ollama - Medical Q&A System\n",
        "\n",
        "## Hardware Setup\n",
        "\n",
        "This project runs on my development machine:\n",
        "\n",
        "- **CPU**: Intel i7-13620H (13th Gen, 10 cores)\n",
        "- **GPU**: RTX 4070 (8GB VRAM)\n",
        "- **RAM**: 32GB DDR5\n",
        "- **Storage**: SSD\n",
        "\n",
        "## Why I Chose Llama3.2:1B\n",
        "\n",
        "I selected **Llama3.2:1B** model based on my hardware:\n",
        "\n",
        "### Memory Optimization\n",
        "- **My RTX 4070 has 8GB VRAM** - perfect for 1B parameter model\n",
        "- Llama3.2:1B needs ~2-3GB VRAM, leaves room for embeddings\n",
        "- Larger models (7B+) would exceed VRAM and use slow CPU\n",
        "\n",
        "### Performance\n",
        "- **Fast inference** (~1-2 seconds per response)\n",
        "- **Low latency** for interactive use\n",
        "- **Works well with RAG** - context is pre-filtered\n",
        "\n",
        "### Local Benefits\n",
        "- **No API costs** - runs entirely local\n",
        "- **Privacy** - all data stays on my machine\n",
        "- **Always available** - no internet dependency\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "```\n",
        "PDF → Text Chunks → Embeddings → FAISS Vector DB\n",
        "                                       ↓\n",
        "Question → Embedding → Search → Context → LLM → Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import faiss\n",
        "import requests\n",
        "import streamlit as st\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "import numpy as np\n",
        "import openai\n",
        "from typing import List, Dict, Any\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Processing\n",
        "\n",
        "Extract text from medical documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_pdf_text(pdf_path):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Load my medical document\n",
        "pdf_text = extract_pdf_text(\"Good-Medical-Practice-2024---English-102607294.pdf\")\n",
        "print(f\"Extracted {len(pdf_text)} characters from PDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Chunking\n",
        "\n",
        "Split document into chunks for retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "# Create chunks from my PDF\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Created {len(text_chunks)} text chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "Create vector representations using SentenceTransformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/multilingual-e5-large')\n",
        "\n",
        "# Generate embeddings for all chunks\n",
        "print(\"Generating embeddings...\")\n",
        "chunk_embeddings = embedding_model.encode(text_chunks)\n",
        "print(f\"Generated {len(chunk_embeddings)} embeddings of dimension {chunk_embeddings[0].shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FAISS Vector Database\n",
        "\n",
        "Build efficient similarity search index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FAISS index\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)\n",
        "\n",
        "# Add embeddings to index\n",
        "index.add(chunk_embeddings.astype('float32'))\n",
        "print(f\"Added {index.ntotal} vectors to FAISS index\")\n",
        "\n",
        "# Save index\n",
        "faiss.write_index(index, \"medical_docs.index\")\n",
        "print(\"Saved FAISS index to file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI Integration\n",
        "\n",
        "Set up GPT-4o-mini for high-quality responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_openai(context, question):\n",
        "    \"\"\"Query OpenAI with context\"\"\"\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a medical expert. Answer based on the provided context.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"OpenAI Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama Local LLM\n",
        "\n",
        "Set up Llama3.2:1B for offline inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_ollama(context, question):\n",
        "    \"\"\"Query Ollama with context\"\"\"\n",
        "    try:\n",
        "        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer based on the context provided:\"\n",
        "        \n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": \"llama3.2:1b\",\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"response\", \"No response from Ollama\")\n",
        "        else:\n",
        "            return f\"Ollama Error: {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"Ollama Connection Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Query Pipeline\n",
        "\n",
        "Complete question-answering workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_query(question, use_openai=True, top_k=3):\n",
        "    \"\"\"Complete RAG pipeline\"\"\"\n",
        "    \n",
        "    # 1. Encode question\n",
        "    question_embedding = embedding_model.encode([question])\n",
        "    \n",
        "    # 2. Search similar chunks\n",
        "    scores, indices = index.search(question_embedding.astype('float32'), top_k)\n",
        "    \n",
        "    # 3. Get relevant context\n",
        "    relevant_chunks = [text_chunks[i] for i in indices[0]]\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "    \n",
        "    # 4. Generate answer\n",
        "    if use_openai:\n",
        "        answer = query_openai(context, question)\n",
        "    else:\n",
        "        answer = query_ollama(context, question)\n",
        "    \n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": relevant_chunks,\n",
        "        \"scores\": scores[0].tolist()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the System\n",
        "\n",
        "Try a medical query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with sample question\n",
        "test_question = \"What are the key principles of good medical practice?\"\n",
        "\n",
        "print(\"Testing with Ollama (Local):\")\n",
        "result_ollama = rag_query(test_question, use_openai=False)\n",
        "print(f\"Answer: {result_ollama['answer']}\")\n",
        "print(f\"Relevance scores: {result_ollama['scores']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Web Interface\n",
        "\n",
        "Deploy the complete system via Streamlit:\n",
        "\n",
        "```bash\n",
        "streamlit run streamlit_app.py --server.port 8878\n",
        "```\n",
        "\n",
        "**Features:**\n",
        "- Choose between OpenAI or Ollama\n",
        "- Upload medical documents\n",
        "- Ask questions and get answers\n",
        "- View source references\n",
        "\n",
        "**My Setup Performance:**\n",
        "- **RTX 4070**: Perfect for Llama3.2:1B inference\n",
        "- **32GB DDR5 RAM**: Fast vector operations\n",
        "- **Response Time**: 1-3 seconds\n",
        "- **Memory Usage**: ~4GB RAM, ~3GB VRAM"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}